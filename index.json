[{"authors":["admin"],"categories":null,"content":"Hello, my name is Shuang Chen (陈爽). I am a fifth-year Ph.D. candidate in the Computer Systems Lab at Cornell University, advised by Prof. José F. Martínez. My research focuses on scheduling and managing interactive services in datacenters. Currently, I am working on improving performance of interactive services by leveraging emerging memory technologies.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"481ee1424e61a542df874f28cd544844","permalink":"https://sc2682cornell.github.io/authors/shuang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shuang/","section":"authors","summary":"Hello, my name is Shuang Chen (陈爽). I am a fifth-year Ph.D. candidate in the Computer Systems Lab at Cornell University, advised by Prof. José F. Martínez. My research focuses on scheduling and managing interactive services in datacenters. Currently, I am working on improving performance of interactive services by leveraging emerging memory technologies.","tags":null,"title":"Shuang Chen","type":"authors"},{"authors":["**Shuang Chen**","Christina Delimitrou","José F. Martínez"],"categories":null,"content":"","date":1555113600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555113600,"objectID":"6bdf7a8f8e90832f72f5be944821c8fa","permalink":"https://sc2682cornell.github.io/publication/parties/","publishdate":"2019-04-13T00:00:00Z","relpermalink":"/publication/parties/","section":"publication","summary":"Multi-tenancy in modern datacenters is currently limited to a single latency-critical, interactive service, running alongside one or more low-priority, best-effort jobs. This limits the efficiency gains from multi-tenancy, especially as most cloud applications progressively shift from batch jobs to services with strict latency requirements. We present PARTIES, a QoS-aware resource manager that enables an arbitrary number of interactive, latency-critical services to share a physical node without QoS violations. PARTIES leverages a set of hardware and software resource partitioning mechanisms to adjust allocations dynamically at runtime, to meet QoS of each co-scheduled workload and maximize throughput for the machine. We evaluate PARTIES on state-of-the-art server platforms across a set of diverse interactive services. Our results show that PARTIES improves throughput under QoS by 61% on average compared to existing resource managers, and that the rate of improvement increases with the number of co-scheduled applications per physical host. ","tags":null,"title":"PARTIES: QoS-Aware Resource Partitioning for Multiple Interactive Services","type":"publication"},{"authors":["**Shuang Chen**","Shay GalOn","Christina Delimitrou","Srilatha Manne","José F. Martínez"],"categories":null,"content":"","date":1506816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506816000,"objectID":"2605986e1ba59e2dc08752fb3b9fca8f","permalink":"https://sc2682cornell.github.io/publication/iiswc17/","publishdate":"2017-10-01T00:00:00Z","relpermalink":"/publication/iiswc17/","section":"publication","summary":"Key-value stores (e.g., Memcached) and web servers (e.g., NGINX) are widely used by cloud providers. As interactive services, they have strict service-level objectives, with typical 99th-percentile tail latencies on the order of a few milliseconds. Unlike average latency, tail latency is more sensitive to changes in usage load and traffic patterns, system configurations, and resource availability. Understanding the sensitivity of tail latency to application and system factors is critical to efficiently design and manage systems for these latency-critical services. We present a comprehensive study of the impact a diverse set of application, hardware, and isolation configurations have on tail latency for two representative interactive services, Memcached and NGINX. Examined factors include input load, thread-level parallelism, request size, virtualization, and resource partitioning. We conduct this study on two server platforms with significant differences in terms of architecture and price points: an Intel Xeon and an ARM-based Cavium ThunderX server. Experimental results show that latency on both platforms is subject to changes of several orders of magnitude depending on application and system settings, with Cavium ThunderX being more sensitive to configuration parameters. ","tags":null,"title":"Workload Characterization of Interactive Cloud Services on Big and Small Server Platforms","type":"publication"},{"authors":["Xiaodong Wang","**Shuang Chen**","Jeff Setter","José F. Martínez"],"categories":null,"content":"","date":1486166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486166400,"objectID":"da92418f8320f927834095768d6d9ebd","permalink":"https://sc2682cornell.github.io/publication/swap/","publishdate":"2017-02-04T00:00:00Z","relpermalink":"/publication/swap/","section":"publication","summary":"Performance isolation is an important goal in server-class environments. Partitioning the last-level cache of a chip multiprocessor (CMP) across co-running applications has proven useful in this regard. Two popular approaches are (a) hardware support for way partitioning, or (b) operating system support for set partitioning through page coloring. Unfortunately, neither approach by itself is scalable beyond a handful of cores without incurring in significant performance overheads. We propose SWAP, a scalable and fine-grained cache management technique that seamlessly combines set and way partitioning. By cooperatively managing cache ways and sets, SWAP (“Set and WAy Partitioning”) can successfully provide hundreds of fine-grained cache partitions for the manycore era. SWAP requires no additional hardware beyond way partitioning. In fact, SWAP can be readily implemented in existing commercial servers whose processors do support hardware way partitioning. In this paper, we prototype SWAP on a 48-core Cavium ThunderX platform running Linux, and show average speedups over no cache partitioning are twice as large as those attained with hardware way partitioning alone.","tags":null,"title":"SWAP: Effective Fine-grain Management of Shared Last-level Caches with Minimum Hardware Support","type":"publication"},{"authors":["**Shuang Chen**","Shunning Jiang","Bingsheng He","Xueyan Tang"],"categories":null,"content":"","date":1467244800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467244800,"objectID":"926c9c2a445e1b657117d507c32d7a8a","permalink":"https://sc2682cornell.github.io/publication/sort/","publishdate":"2016-06-30T00:00:00Z","relpermalink":"/publication/sort/","section":"publication","summary":"Hardware evolution has been one of the driving factors for the redesign of database systems. Recently, approximate storage emerges in the area of computer architecture. It trades off precision for better performance and/or energy consumption. Previous studies have demonstrated the benefits of approximate storage for applications that are tolerant to imprecision such as image processing. However, it is still an open question whether and how approximate storage can be used for applications that do not expose such intrinsic tolerance. In this paper, we study one of the most basic operations in database--sorting on a hybrid storage system with both precise storage and approximate storage. Particularly, we start with a study of three common sorting algorithms on approximate storage. Experimental results show that a 95% sorted sequence can be obtained with up to 40% reduction in total write latencies. Thus, we propose an approx-refine execution mechanism to improve the performance of sorting algorithms on the hybrid storage system to produce precise results. Our optimization gains the performance benefits by offloading the sorting operation to approximate storage, followed by an efficient refinement to resolve the unsortedness on the output of the approximate storage. Our experiments show that our approx-refine can reduce the total memory access time by up to 11%. These studies shed light on the potential of approximate hardware for improving the performance of applications that require precise results.","tags":null,"title":"A Study of Sorting Algorithms on Approximate Memory","type":"publication"},{"authors":["Naifeng Jing","**Shuang Chen**","Shunning Jiang","Li Jiang","Chao Li","Xiaoyao Liang"],"categories":null,"content":"","date":1437523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437523200,"objectID":"47a47552678617c867480c5ac95d0e94","permalink":"https://sc2682cornell.github.io/publication/bank/","publishdate":"2015-07-22T00:00:00Z","relpermalink":"/publication/bank/","section":"publication","summary":"Modern General Purpose Graphic Processing Unit (GPGPU) demands a large Register File (RF), which is typically organized into multiple banks to support the massive parallelism. Although heavy banking benefits RF throughput, its associated area and energy costs with diminishing performance gains greatly limit future RF s-caling. In this paper, we propose an improved RF design with a bank stealing technique, which enables a high RF throughput with compact area. By deeply investigating the GPGPU microarchitecture, we identify the deficiency in the state-of-the-art RF designs as the bank conflict problem, while the majority of conflicts can be eliminated leveraging the fact that the highly-banked RF oftentimes experiences under-utilization. This is especially true in GPGPU where multiple ready warps are available at the scheduling stage with their operands to be wisely coordinated. Our lightweight bank stealing technique can opportunistically fill the idle banks for better operand service, and the average GPGPU performance can be improved under smaller energy budget with significant area saving, which makes it promising for sustainable RF scaling.","tags":null,"title":"Bank stealing for conflict mitigation in GPGPU Register File","type":"publication"}]